{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbfbc6a",
   "metadata": {},
   "source": [
    "# Other Techniques Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc676ff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q \"google-genai>=1.0.0\"  # Install the Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b25fb9",
   "metadata": {},
   "source": [
    "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc937084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from google import genai\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "questions = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/questions.txt\").text.split('\\n')\n",
    "content = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/content.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a68ba2",
   "metadata": {},
   "source": [
    "## Token-Aware Chunking and Batching Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    Answer each of the inputted questions using the information provided to you in the prompt. Each answer should be a **single** string in the JSON response.\n",
    "    There should be **exactly** one answer for each inputted question, no more, no less. \n",
    "    * **Accuracy and Precision:** Provide direct, factual answers. **Do not** create or merge any of the questions.\n",
    "    * **Source Constraint:** Use *only* information explicitly present in the transcript. Do not infer, speculate, or bring in outside knowledge.\n",
    "    * **Completeness:** Ensure each answer fully addresses the question, *to the extent possible with the given transcript*.\n",
    "    * **Missing Information:** If the information required to answer a question is not discussed or cannot be directly derived from the transcript, respond with \"N/A\".\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "input_token_limit = client.models.get(model = model_name).input_token_limit # Retrieving the input token limit of the specified model\n",
    "\n",
    "answers = {}\n",
    "\n",
    "# Beginning with attempting to make an API call with the entire content & questions, if this fails we can break it up as appropriate.\n",
    "queue = [(content, questions)]\n",
    "\n",
    "while len(queue) > 0:\n",
    "    curr_content, curr_questions = queue.pop(0)\n",
    "\n",
    "    # Checking whether the input token limit is exceeded.\n",
    "    input_tokens_required = client.count_tokens(\n",
    "        model = model_name,\n",
    "        contents = [curr_content, curr_content]\n",
    "    )\n",
    "    if input_tokens_required > input_token_limit:\n",
    "        # In this case we know that an API call with the current content will exceed the input token limit for the current model.\n",
    "        # If this is the case, we split the content in half so each API call processes half of the content.\n",
    "        chunked_content = [curr_content[0 : len(curr_content)//2 + 1], curr_content[len(curr_content)//2 + 1 : len(curr_content)]]\n",
    "        queue.append((chunked_content[0], curr_questions))\n",
    "        queue.append((chunked_content[1], curr_questions))\n",
    "        continue\n",
    "\n",
    "    # Making the API call to the Gemini model\n",
    "    reponse = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_prompt,\n",
    "            response_mime_type = \"application/json\"\n",
    "            response_schema = list[str]\n",
    "        ),\n",
    "        contents=[f'Content:\\n{curr_content}', f'\\nQuestion:\\n{curr_questions}']\n",
    "    )\n",
    "\n",
    "    # Checking the finish reason of token generation, anything other than 'STOP' is unnatural.\n",
    "    if response.candidates[0].finish_reason == types.FinishReason.MAX_TOKENS:\n",
    "        # In this case we know that token generation finished due to max token limit being exceeded, therefore we likely have not recieved a full answer.\n",
    "        # We will therefore retry the API call but split the questions into batches of half the sizes to reduce the output.\n",
    "        batch1, batch2 = curr_questions[0 : len(curr_questions)//2 + 1], curr_content[len(curr_questions)//2 + 1 : len(curr_questions)]\n",
    "        queue.append((curr_content, batch1))\n",
    "        queue.append((curr_content, batch2))\n",
    "        continue\n",
    "\n",
    "    response_parsed = json.loads(response.text)\n",
    "\n",
    "    for i in range(len(curr_questions)):\n",
    "        answers[curr_questions[i]] = response_parsed[i]\n",
    "\n",
    "print(\"Answers\")\n",
    "for key in (answers.keys()):\n",
    "    print (f\"{key}\\n\\t{answers[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a5fe2",
   "metadata": {},
   "source": [
    "## Semantic Batching and Chunking Example\n",
    "\n",
    "Step 1: Generating chunks semantically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Splitting sentences and stripping excess detail\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "model = SentenceTransformer(transformer_model)\n",
    "\n",
    "# Creating sentence embeddings using the SentenceTransformer model\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# Calculating the similarity between adjacent embeddings\n",
    "similarities = []\n",
    "for i in range(len(sentence_embeddings) - 1):\n",
    "    # Reshape for cosine_similarity: (1, n_features) for each vector\n",
    "    s1 = sentence_embeddings[i].reshape(1, -1)\n",
    "    s2 = sentence_embeddings[i+1].reshape(1, -1)\n",
    "    similarity = cosine_similarity(s1, s2)[0][0]\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Calculating a threshold value for cosine similarity.\n",
    "mean = np.mean(similarities)\n",
    "std_dev = np.std(similarities)\n",
    "similarity_threshold = mean - (std_dev * threshold_factor)\n",
    "\n",
    "boundaries = [0]\n",
    "current_chunk_start_pos = 0\n",
    "for i in range(len(similarities)):\n",
    "    # Checking if there is a natural boundary.\n",
    "    if similarities[i] < similarity_threshold and (i + 1) - current_chunk_start_pos >= min_sentences_per_chunk:\n",
    "        boundaries.append(i+1)\n",
    "        current_chunk_start_pos = i + 1\n",
    "    elif (i+1) - current_chunk_start_pos >= max_sentences_per_chunk:\n",
    "        boundaries.append(i+1)\n",
    "        current_chunk_start_pos = i + 1\n",
    "        \n",
    "# Adding the end point if it has not already been added\n",
    "if boundaries[-1] != len(similarities) + 1:\n",
    "    boundaries.append(len(similarities) + 1)\n",
    "\n",
    "# Creating the chunks based on the boundaries.\n",
    "content_chunks = []\n",
    "for i in range(len(boundaries) - 1):\n",
    "    content_chunks.append(\" \".join(sentences[boundaries[i] : boundaries[i+1]]))\n",
    "\n",
    "print (content_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9662e5e",
   "metadata": {},
   "source": [
    "Step 2: Generating batches semantically based on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835685a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a batch for each chunk. Each batch only contains the questions for its respective chunks.\n",
    "question_batches = [[] for _ in range(len(chunked_content))]\n",
    "\n",
    "# Creating embeddings for each question.\n",
    "question_embeddings = model.encode(questions)\n",
    "# Creating an embeddings for each chunk - not each sentence in a chunk.\n",
    "chunk_embeddings = model.encode(chunked_content)\n",
    "\n",
    "for i in range(len(question_embeddings)):\n",
    "    # Calculating the similarity to each chunk.\n",
    "    chunk_similarity = cosine_similarity(question_embeddings[i].reshape(1, -1), chunk_embeddings)[0]\n",
    "    # Finding the most similar chunk and adding the question to its batch.\n",
    "    most_similar_chunk = np.argmax(chunk_similarity)\n",
    "    question_batches[most_similar_chunk].append(questions[i])\n",
    "\n",
    "print (question_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
