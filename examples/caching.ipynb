{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbfbc6a",
   "metadata": {},
   "source": [
    "# Caching Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc676ff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q \"google-genai>=1.0.0\"  # Install the Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b25fb9",
   "metadata": {},
   "source": [
    "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc937084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from google import genai\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "questions = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/questions.txt\").text.split('\\n')\n",
    "content = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/content.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a68ba2",
   "metadata": {},
   "source": [
    "## Caching example 1 - Implicit Caching\n",
    "\n",
    "In this example, we can see how to check whether implicit caching has had an effect on our API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(thinking_config=types.ThinkingConfig(thinking_budget=0)),\n",
    "    contents=[f'Content:\\n{content}', f'\\nQuestion:\\n{questions[-1]}']\n",
    ")\n",
    "\n",
    "print(f'Total input tokens: {response.usage_metadata.prompt_token_count}')\n",
    "print(f'Total input tokens from cache: {response.usage_metadata.cached_content_token_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4a5f7",
   "metadata": {},
   "source": [
    "Now asking the question again, check if there is any different in `cached_content_token_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(thinking_config=types.ThinkingConfig(thinking_budget=0)),\n",
    "    contents=[f'Content:\\n{content}', f'\\nQuestion:\\n{questions[-1]}']\n",
    ")\n",
    "\n",
    "print(f'Total input tokens: {response.usage_metadata.prompt_token_count}')\n",
    "print(f'Total input tokens from cache: {response.usage_metadata.cached_content_token_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a5fe2",
   "metadata": {},
   "source": [
    "## Caching example 2 - Explicit Caching\n",
    "In this example, we demonstrate how explicit caching can be done with the Gemini Python SDK. In particular, we upload the entire transcript to the cache, which can then be used in future queries, rather than having to add the transcript to the `contents` parameter every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the content (the transcript) to the cache.\n",
    "cache = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.CreateCachedContentConfig(\n",
    "        display_name='transcript_content', # This allows for the cache to easily be accessed and referred to.\n",
    "        system_instruction=system_prompt,\n",
    "        contents=[content], # The actual contents of the cache. This could also contain other media types, such as videos and photos.\n",
    "        ttl=\"300s\", # The TTL (time to live) of the cache, this limits how long the cache is accessible for.\n",
    "    )\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "    system_instruction=system_prompt,\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "        cached_content = cache.name # Here we referred to the previously cached transcript.\n",
    "    ),\n",
    "    contents=[f'\\nQuestion:\\n{questions[-2]}'] # Only the questions are passed here and not the transcript.\n",
    ")\n",
    "\n",
    "print(f'Total input tokens: {response.usage_metadata.prompt_token_count}')\n",
    "print(f'Total input tokens from cache: {response.usage_metadata.cached_content_token_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
