{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bab83e",
   "metadata": {},
   "source": [
    "# Gemini API: Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071ec54",
   "metadata": {},
   "source": [
    "Chunking is the opposite of batching and describes the process of breaking down a large input into multiple smaller pieces, referred to as chunks. Once again taking a real life example, imagine you are eating a steak, it is too large to eat in a single mouthful so instead you cut it into pieces and eat a piece at a time.\n",
    "\n",
    "In the context of LLMs, models have token limits, which restrict the amount of data that can be injested in a single API call, so developers must be aware of the amount of content being transmitted to the model.\n",
    "\n",
    "There are also other benefits to using chunking, including:\n",
    "* Improved Performance - If an error occurs during API calls, only the individual chunk needs to be reprocessed rather than the entire input, which is significantly quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bfaac",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe4ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "content = \"\"\n",
    "with open(\"demo_files/content.txt\", 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "questions = []\n",
    "with open(\"demo_files/questions.txt\", 'r', encoding='utf-8') as file:\n",
    "    for question in file:\n",
    "        questions.append(question.strip())\n",
    "# Only getting the first question for this example.\n",
    "question = questions[0]\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62552a",
   "metadata": {},
   "source": [
    "## Chunking Techniques\n",
    "\n",
    "Since the Gemini LLM is natively multimodal, the various media types will require custom chunking strategies. In this example, only simple text chunking methods are demonstrated, however other techniques are discussed later.\n",
    "\n",
    "It is also worth noting that the Google Gemini Models come with large context windows (1,048,576 input tokens for 2.5 Pro and Flash), so chunking may not be needed in some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed85e3f",
   "metadata": {},
   "source": [
    "### Fixed Chunking\n",
    "\n",
    "The example content used in this demonstration has 53,405 characters, which is less that that of the input token limit, however for this example imagine that the token limit is ~10,000 characters.\n",
    "\n",
    "In fixed chunking, the content is split into non-overlapping chunks that are each of 10,000 characters.\n",
    "\n",
    "TODO: Add visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_char_size = 10000\n",
    "chunked_content = []\n",
    "chunk_count = math.ceil(len(content) / chunk_char_size)\n",
    "\n",
    "for i in range(chunk_count):\n",
    "    chunk_start_pos = i * chunk_char_size\n",
    "    chunk_end_pos = min(chunk_start_pos + chunk_char_size, len(content))\n",
    "    chunked_content.append(content[chunk_start_pos : chunk_end_pos])\n",
    "\n",
    "print(f'Number of chunks: {len(chunked_content)}')\n",
    "# TODO: Add API calls to Gemini to demonstrate using the chunks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e89e35",
   "metadata": {},
   "source": [
    "### Sliding Window Chunking\n",
    "\n",
    "One disadvantage of fixed chunking is that it may break context at arbitrary positions, meaning that important information can get split between chunkings, i.e. half a sentence is in the first chunk and half is in the second, meaning neither chunks are able to fully answer a question about the sentence. The sliding window approach addresses this by using overlapping chunks, so each chunk shares some content with the previous chunk (this is called the window).\n",
    "\n",
    "One disadvantage is that this increases the number of chunks required, causing an increase in the amount of API calls needed.\n",
    "\n",
    "TODO: Add visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dabe464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 8\n"
     ]
    }
   ],
   "source": [
    "chunk_char_size = 10000\n",
    "window_char_size = 2500\n",
    "\n",
    "chunked_content = []\n",
    "chunk_count = math.ceil(len(content) / (chunk_char_size - window_char_size))\n",
    "\n",
    "for i in range(chunk_count):\n",
    "    chunk_start_pos = i * (chunk_char_size - window_char_size)\n",
    "    chunk_end_pos = min(chunk_start_pos + chunk_char_size, len(content))\n",
    "    chunked_content.append(content[chunk_start_pos : chunk_end_pos])\n",
    "\n",
    "print(f'Number of chunks: {len(chunked_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c396de3",
   "metadata": {},
   "source": [
    "### Other Chunking Methods\n",
    "\n",
    "TODO: these methods increase in complexity, requiring more time to complete.\n",
    "\n",
    "\n",
    "This list is not exhaustive and a combination of all the techniques or a different technique altogether may perform better depending on the use case.\n",
    "\n",
    "- Text\n",
    "    - Semantic Chunking: This involves breaking down the content into chunks based on semantic meaning. Here sentences are grouped together if they discuss similar topics, making it more likely that a question can be answered entirely by a single chunk. One implementation of this would involve calcuating the embeddings of each sentence using `SentenceTransformer` and then computing the cosine similarity of each sentence.\n",
    "- Audio\n",
    "    - Fixed/Sliding Window Chunking by duration: For audio, similar techniques can be used, rather than chunking by the number of sentences, the input can be split based on time duration.\n",
    "    - Text Methods via Transcripts: Models such as Google's Speech-to-Text or OpenAI's Whisper can be used to create a transcript of a file. This allows the text based methods (fixed/sliding window/semantic) to be used, as both models also provide timestamps for when each sentence occured.\n",
    "    - Speaker Diarization: Analysis can also be completed on the audio itself to detect when the speaker changes or there is a natural break in speech, which can also often act as good chunking positions. One common library for this use is `pyannote.audio`.\n",
    "- Video\n",
    "    - Audio Methods: Each of the methods mentioned when discussing the audio techniques can also be used for video content by isolating the audio.\n",
    "    - Visual Content: Finally, you could analyse the pictures shown in the video to detect a change in the scene, for example a camera cut, which could provide a good chunking position. A useful library for this is `PySceneDetect` which detects when visual scene changes occur.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
