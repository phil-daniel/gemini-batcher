{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbfbc6a",
   "metadata": {},
   "source": [
    "# Batching Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc676ff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q \"google-genai>=1.0.0\"  # Install the Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b25fb9",
   "metadata": {},
   "source": [
    "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc937084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from google import genai\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "questions = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/questions.txt\").text.split('\\n')\n",
    "content = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/content.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a68ba2",
   "metadata": {},
   "source": [
    "## Batching example 1 - no batching (baseline)\n",
    "\n",
    "In this example, the baseline number of tokens required to answer the first five questions is calculated. Each question is sent to the model sequentially, along with the entire transcript.\n",
    "\n",
    "The response is returned in JSON format for easier comparison to the batched example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Answer the questions using *only* the content provided, with each answer being a different string in the JSON response.\"\n",
    "\n",
    "total_input_tokens_no_batching = 0\n",
    "total_output_tokens_no_batching = 0\n",
    "\n",
    "for question in questions[:5]:\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=list[str],\n",
    "            system_instruction=system_prompt,\n",
    "        ),\n",
    "        contents=[f'Content:\\n{content}', f'\\nQuestion:\\n{question}']\n",
    "    )\n",
    "    total_input_tokens_no_batching += response.usage_metadata.prompt_token_count\n",
    "    total_output_tokens_no_batching += response.usage_metadata.candidates_token_count\n",
    "\n",
    "print (f'Total input tokens used with no batching: {total_input_tokens_no_batching}')\n",
    "print (f'Total output tokens used with no batching: {total_output_tokens_no_batching}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a5fe2",
   "metadata": {},
   "source": [
    "## Batching example 2 - with batching\n",
    "In this example, the model is asked the same five questions, but rather than being asked individually, they are answered all at once. This results in a significant reduction in the number of input tokens used as the model is only provided with the large content once rather than five times.\n",
    "\n",
    "The response is returned in JSON format to allow for easier separation of each question's answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Answer the questions using *only* the content provided, with each answer being a different string in the JSON response.\"\n",
    "\n",
    "batched_questions = (\"\\n\").join(questions[:5])\n",
    "\n",
    "batched_response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=list[str],\n",
    "        system_instruction=system_prompt,\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0,)\n",
    "    ),\n",
    "    contents=[f'Content:\\n{content}', f'\\nQuestions:\\n{batched_questions}']\n",
    ")\n",
    "\n",
    "answers = batched_response.text\n",
    "batched_answers = json.loads(answers.strip())\n",
    "\n",
    "total_input_tokens_with_batching = batched_response.usage_metadata.prompt_token_count\n",
    "total_output_tokens_with_batching = batched_response.usage_metadata.candidates_token_count\n",
    "\n",
    "print (f'Total input tokens used with batching: {total_input_tokens_with_batching}')\n",
    "print (f'Total output tokens used with batching: {total_output_tokens_with_batching}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
