{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08bc87d",
   "metadata": {},
   "source": [
    "# Gemini API: Batching\n",
    "\n",
    "Batching describes the process of combining multiple individual API calls together into single API call. Imagine you need to buy three things from a shop, rather than going to the shop three separate times, buying one item each time, it would be more efficient to only go to the shop once, getting everything you need. The technique can provide multiple benefits, including:\n",
    "* Reduced latency - Rather than having to make repeated HTTP calls, only a single one must be made, reducing latency. In addition, since many LLM APIs have rate limits, the number of requests which can be made may be limited. \n",
    "* Improved cost efficiency - In some situations, combining your inputs into a single API call can reduce the number of tokens required. For example, given a paragraph costing 400 tokens to process, and 5 questions each costing 10 tokens, asking the questions one at a time would take ≈ (400 + 10) * 5 = 2050 tokens, whereas batching the questions would only take ≈ 400 + (10 * 5) = 450 tokens, giving a signficant improvement. An example of this is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36081c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "086bf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "content = \"\"\n",
    "with open(\"content.txt\", 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "questions = []\n",
    "with open(\"questions.txt\", 'r', encoding='utf-8') as file:\n",
    "    for question in file:\n",
    "        questions.append(question.strip())\n",
    "# Only getting the first 5 questions for this example.\n",
    "questions = questions[:5]\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb2e97",
   "metadata": {},
   "source": [
    "### No Batching\n",
    "\n",
    "In this example, we provide the model with a large block of content and ask it five questions based on that content, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d6a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens used with no batching: 66454\n",
      "Total output tokens used with no batching: 638\n"
     ]
    }
   ],
   "source": [
    "total_input_tokens_no_batching = 0\n",
    "total_output_tokens_no_batching = 0\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    Answer the question using the content provided.\n",
    "    * **Accuracy and Precision:** Provide direct, factual answers.\n",
    "    * **Source Constraint:** Use *only* information explicitly present in the content. Do not infer, speculate, or bring in outside knowledge.\n",
    "    * **Completeness:** Ensure each answer fully addresses the question, *to the extent possible with the given transcript*.\n",
    "\"\"\"\n",
    "\n",
    "for question in questions:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_prompt\n",
    "        ),\n",
    "        contents=[f'Content:\\n{content}', f'\\nQuestion:\\n{question}']\n",
    "    )\n",
    "    total_input_tokens_no_batching += response.usage_metadata.prompt_token_count\n",
    "    total_output_tokens_no_batching += response.usage_metadata.candidates_token_count\n",
    "\n",
    "print (f'Total input tokens used with no batching: {total_input_tokens_no_batching}')\n",
    "print (f'Total output tokens used with no batching: {total_output_tokens_no_batching}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71714b",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "In this batched example, we ask model the same questions, but ask them all at once. A slightly modified system prompt is also used, requiring the API to respond in JSON format. This makes is significantly easier to parse the response and split it into the individual answers.\n",
    "\n",
    "The signficant redunction in the number of input tokens used in this is example is because the large content only needs to be provided to the API once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71311f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens used with batching: 13360\n",
      "Total output tokens used with batching: 367\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "    Answer the questions using the content provided.\n",
    "    * **Accuracy and Precision:** Provide direct, factual answers.\n",
    "    * **Source Constraint:** Use *only* information explicitly present in the content. Do not infer, speculate, or bring in outside knowledge.\n",
    "    * **Completeness:** Ensure each answer fully addresses the question, *to the extent possible with the given transcript*.\n",
    "    Respond using a JSON array, where each element is an answer to a question.\n",
    "    e.g. {\n",
    "        {'answer' : \"Answer to question 1\"},\n",
    "        {'answer' : \"Answer to question 2\"},\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "batched_questions = (\"\\n\").join(questions)\n",
    "\n",
    "batched_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        system_instruction=system_prompt\n",
    "    ),\n",
    "    contents=[f'Content:\\n{content}', f'\\nQuestions:\\n{batched_questions}']\n",
    ")\n",
    "\n",
    "batched_answer = json.loads(response.text)\n",
    "batched_answer = [ans['answer'] for ans in batched_answer]\n",
    "\n",
    "total_input_tokens_with_batching = response.usage_metadata.prompt_token_count\n",
    "total_output_tokens_with_batching = response.usage_metadata.candidates_token_count\n",
    "\n",
    "print (f'Total input tokens used with batching: {total_input_tokens_with_batching}')\n",
    "print (f'Total output tokens used with batching: {total_output_tokens_with_batching}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b193087",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When batching, it is important to take into account the context limit of the model used. This is because:\n",
    "* Batching too many questions together may increase the number of input tokens over the model's limit, causing errors. One solution to this is to also break down the content into multiple chunks, which is called chunking.\n",
    "* Batching too many questions may increase the number of output tokens over the model's limit, meaning that not all of the questions are answered.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
